{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load - Wrangle - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:\\My Drive\\Code\\springboard_proj_nlp_nytimes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as plty\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "# Textacy\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#https://realpython.com/natural-language-processing-spacy-python/\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "join_comments = True\n",
    "join_raw_article = True\n",
    "\n",
    "# Set Project Root\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "print(os.path.abspath(PROJ_ROOT))\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "# %run \"$BASE_DIR/settings.py\"\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# Set to Reload all custom packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "from collections import Counter\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Article Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comments = pd.read_csv('..//data//raw//nyt-comments-2020.csv',parse_dates=['createDate','updateDate','approveDate'])\n",
    "# df_comments.sort_values(by='createDate',inplace=True,ascending=True)\n",
    "\n",
    "# ## Full Article Verbose / Write to file\n",
    "# df_comments.to_csv('..//data//processed//df_comments.csv')\n",
    "# df_comments.to_pickle('..//data//processed//df_comments.pkl')\n",
    "\n",
    "# ## Sample Article Verbose / Write to file\n",
    "# # df_sample = df_comments.sample(frac=0.05)\n",
    "# # df_sample.to_csv('..//data//processed//df_comments_sample.csv')\n",
    "# # df_sample.to_pickle('..//data//processed//df_comments_sample.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NYT Articles\n",
    "Will join with a more verbose set of data\n",
    "(This is the whole list (not train and test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All Articles\n",
    "df_articles = pd.read_csv('..//data//raw//nyt-articles-2020.csv',parse_dates=['pub_date'])\n",
    "df_articles.sort_values(by='pub_date',inplace=True,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Enhanced Articles\n",
    "\n",
    "Pull print section, print_page, lead_paragraph, and headlines to provide possible more text features that are not present in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: ..//data//raw//2020-01-articles.pickle\n",
      "Filename: ..//data//raw//2020-02-articles.pickle\n",
      "Filename: ..//data//raw//2020-03-articles.pickle\n",
      "Filename: ..//data//raw//2020-04-articles.pickle\n",
      "Filename: ..//data//raw//2020-05-articles.pickle\n",
      "Filename: ..//data//raw//2020-06-articles.pickle\n",
      "Filename: ..//data//raw//2020-07-articles.pickle\n",
      "Filename: ..//data//raw//2020-08-articles.pickle\n",
      "Filename: ..//data//raw//2020-09-articles.pickle\n",
      "Filename: ..//data//raw//2020-10-articles.pickle\n",
      "Filename: ..//data//raw//2020-11-articles.pickle\n",
      "Filename: ..//data//raw//2020-12-articles.pickle\n"
     ]
    }
   ],
   "source": [
    "# build file list matching the pickle file name pattern\n",
    "filelist = ['..//data//raw//2020-'+ str(x).zfill(2) + '-articles.pickle' for x in range(1,13)]\n",
    "\n",
    "df_articles_verbose = pd.DataFrame()\n",
    "df_temp = pd.DataFrame()\n",
    "\n",
    "# For each one of the file names\n",
    "for filename in filelist[0:13]:\n",
    "    print('Filename: ' + filename)\n",
    "    \n",
    "    # Get large articles file\n",
    "    df_json = pd.read_pickle(filename)\n",
    "    df_json = df_json[['uri','print_section','print_page','lead_paragraph','headline.main','byline.person','byline.organization','headline.sub','text','web_url']]\n",
    "    \n",
    "    df_temp = df_temp.append(df_json,ignore_index=True)\n",
    " \n",
    "  \n",
    "    del(df_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join enhanced article data to original article df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Articles Back\n",
    "df_articles_verbose = pd.merge(df_articles, df_temp, how='left', left_on='uniqueID',right_on='uri',suffixes=['_left','_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16787 entries, 0 to 16786\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype              \n",
      "---  ------               --------------  -----              \n",
      " 0   newsdesk             16787 non-null  object             \n",
      " 1   section              16787 non-null  object             \n",
      " 2   subsection           5693 non-null   object             \n",
      " 3   material             16787 non-null  object             \n",
      " 4   headline             16787 non-null  object             \n",
      " 5   abstract             16784 non-null  object             \n",
      " 6   keywords             16786 non-null  object             \n",
      " 7   word_count           16787 non-null  int64              \n",
      " 8   pub_date             16787 non-null  datetime64[ns, UTC]\n",
      " 9   n_comments           16787 non-null  int64              \n",
      " 10  uniqueID             16787 non-null  object             \n",
      " 11  uri                  16787 non-null  object             \n",
      " 12  print_section        11598 non-null  object             \n",
      " 13  print_page           11598 non-null  object             \n",
      " 14  lead_paragraph       16787 non-null  object             \n",
      " 15  headline.main        16787 non-null  object             \n",
      " 16  byline.person        16787 non-null  object             \n",
      " 17  byline.organization  627 non-null    object             \n",
      " 18  headline.sub         0 non-null      object             \n",
      " 19  text                 16787 non-null  object             \n",
      " 20  web_url              16787 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(18)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_articles_verbose.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_verbose.drop(columns=['headline.sub','byline.organization','subsection'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full Article Verbose / Write to file\n",
    "df_articles_verbose.to_csv('..//data//processed//df_articles_all.csv')\n",
    "df_articles_verbose.to_pickle('..//data//processed//df_articles_all.pkl')\n",
    "\n",
    "# ## Sample Article Verbose / Write to file\n",
    "# df_sample = df_articles_verbose.sample(frac=0.05)\n",
    "# df_sample.to_csv('..//data//processed//df_articles_sample.csv')\n",
    "# df_sample.to_pickle('..//data//processed//df_articles_sample.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling - Entry point for review (skips raw loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample or Full Set of Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_pickle('..//data//processed//df_articles_all.pkl')\n",
    "\n",
    "# # Drop constant columns, userTitle is 99% empty\n",
    "# df_comments.drop(columns=['status','trusted','recommendedFlag','isAnonymous','userTitle'],inplace=True)\n",
    "\n",
    "# Convert Timestamps where necessary\n",
    "df_articles['pub_date'] = pd.to_datetime(df_articles['pub_date'])\n",
    "# df_comments['createDate'] = pd.to_datetime(df_comments['createDate'])\n",
    "# df_comments['updateDate'] = pd.to_datetime(df_comments['updateDate'])\n",
    "# df_comments['approveDate'] = pd.to_datetime(df_comments['approveDate'])\n",
    "\n",
    "# NAN Drops\n",
    "# Drop Keyword Columns that are not str\n",
    "df_articles.dropna(subset=['keywords'],inplace=True) # Key for teh NER to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16786 entries, 0 to 16786\n",
      "Data columns (total 18 columns):\n",
      " #   Column          Non-Null Count  Dtype              \n",
      "---  ------          --------------  -----              \n",
      " 0   newsdesk        16786 non-null  object             \n",
      " 1   section         16786 non-null  object             \n",
      " 2   material        16786 non-null  object             \n",
      " 3   headline        16786 non-null  object             \n",
      " 4   abstract        16783 non-null  object             \n",
      " 5   keywords        16786 non-null  object             \n",
      " 6   word_count      16786 non-null  int64              \n",
      " 7   pub_date        16786 non-null  datetime64[ns, UTC]\n",
      " 8   n_comments      16786 non-null  int64              \n",
      " 9   uniqueID        16786 non-null  object             \n",
      " 10  uri             16786 non-null  object             \n",
      " 11  print_section   11597 non-null  object             \n",
      " 12  print_page      11597 non-null  object             \n",
      " 13  lead_paragraph  16786 non-null  object             \n",
      " 14  headline.main   16786 non-null  object             \n",
      " 15  byline.person   16786 non-null  object             \n",
      " 16  text            16786 non-null  object             \n",
      " 17  web_url         16786 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(15)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsdesk                                                      Metro\n",
       "section                                                    New York\n",
       "material                                                       News\n",
       "headline          More People Are Dying on New York City’s Stree...\n",
       "abstract          More cyclists were killed last year than in an...\n",
       "keywords          ['Traffic Accidents and Safety', 'Bicycles and...\n",
       "word_count                                                     1186\n",
       "pub_date                                  2020-01-01 18:19:38+00:00\n",
       "n_comments                                                      267\n",
       "uniqueID          nyt://article/edd3d743-b68d-50ee-9af9-b6ee1200...\n",
       "uri               nyt://article/edd3d743-b68d-50ee-9af9-b6ee1200...\n",
       "print_section                                                     A\n",
       "print_page                                                       16\n",
       "lead_paragraph    As New York City tackled the stubborn problem ...\n",
       "headline.main     More People Are Dying on New York City’s Stree...\n",
       "byline.person     [{'firstname': 'Emma', 'middlename': 'G.', 'la...\n",
       "text              As New York City tackled the stubborn problem ...\n",
       "web_url           https://www.nytimes.com/2020/01/01/nyregion/ny...\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.iloc[20].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Month and drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "df_articles['month'] = df_articles['pub_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>16786.0</td>\n",
       "      <td>1300.961813</td>\n",
       "      <td>944.801861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>874.0</td>\n",
       "      <td>1183.0</td>\n",
       "      <td>1502.0</td>\n",
       "      <td>15619.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_comments</th>\n",
       "      <td>16786.0</td>\n",
       "      <td>297.056714</td>\n",
       "      <td>513.419176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>8987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>16786.0</td>\n",
       "      <td>6.320743</td>\n",
       "      <td>3.437056</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count         mean         std  min    25%     50%     75%  \\\n",
       "word_count  16786.0  1300.961813  944.801861  0.0  874.0  1183.0  1502.0   \n",
       "n_comments  16786.0   297.056714  513.419176  1.0   21.0    87.0   323.0   \n",
       "month       16786.0     6.320743    3.437056  1.0    3.0     6.0     9.0   \n",
       "\n",
       "                max  \n",
       "word_count  15619.0  \n",
       "n_comments   8987.0  \n",
       "month          12.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Text Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy NER\n",
    "for ent in piano_class_doc.ents:\n",
    "...     print(ent.text, ent.start_char, ent.end_char,\n",
    "...           ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get part of speech \n",
    "def get_pos_tag_len(text, pos_type):\n",
    "    doc = nlp(text)\n",
    "    pos = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == pos_type:\n",
    "            pos.append(token.pos_)\n",
    "    pos = Counter(pos)\n",
    "    return (pos[pos_type])\n",
    "\n",
    "\n",
    "#'PROPN', 'PUNCT', 'DET', 'NOUN', 'VERB', 'ADV', 'SCONJ', 'PRON', 'ADP', 'ADJ', 'AUX', 'NUM', 'INTJ#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Entities\n",
    "def get_ent_tag_len(text,ent_label):\n",
    "    ents = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == ent_label:\n",
    "            ents.append(ent.text)\n",
    "\n",
    "    ents = Counter(ents)\n",
    "    return (len(ents))\n",
    "\n",
    "# 'PERSON'\n",
    "# 'NORP' - Nationalities, relgious, political groups\n",
    "# 'FAC' - Buildings, airports, highways, bridges, etc\n",
    "# 'ORG' - Companies, Agencies, Institutions\n",
    "# 'GPE' - Countries, Cities, States\n",
    "# 'PRODUCT' - Objects, vehicles, foods, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make POS Features\n",
    "Count parts of speech by type (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spacy Nouns\n",
    "df_articles['TEXT_Keywords_POS_NOUN'] = df_articles['keywords'].apply(get_pos_tag_len,pos_type='NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PropN\n",
    "df_articles['TEXT_Keywords_POS_PNOUN'] = df_articles['keywords'].apply(get_pos_tag_len,pos_type='PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Ent Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Spacy Entities of Organizations\n",
    "df_articles['TEXT_Keywords_ENT_ORG'] = df_articles['keywords'].apply(get_ent_tag_len,ent_label='ORG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Spacy Entities of Nationalities, Religious, Political Group\n",
    "df_articles['TEXT_Keywords_ENT_NORP'] = df_articles['keywords'].apply(get_ent_tag_len,ent_label='NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Spacy Entities Buildings, Airports, Highways\n",
    "df_articles['TEXT_Keywords_ENT_FAC'] = df_articles['keywords'].apply(get_ent_tag_len,ent_label='FAC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Spacy Entities Countries, Cities, States\n",
    "df_articles['TEXT_Keywords_ENT_GPE'] = df_articles['keywords'].apply(get_ent_tag_len,ent_label='GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spacy entities Locations\n",
    "df_articles['TEXT_Keywords_ENT_LOC'] = df_articles['keywords'].apply(get_ent_tag_len,ent_label='LOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spacy entities locations People\n",
    "df_articles['TEXT_Keywords_ENT_PERSON'] = df_articles['keywords'].apply(get_ent_tag_len,ent_label='PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Split to Binary Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_split = 350\n",
    "cut_labels = [0,1]\n",
    "cut_bins = [0,comment_split,max(df_articles['n_comments'])]\n",
    "\n",
    "# Process Square Footage\n",
    "# Enable Hue of building size\n",
    "df_articles['comment_isHigh'] = pd.cut(df_articles['n_comments'], bins=cut_bins, labels=cut_labels)\n",
    "df_articles.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.sort_values(by='n_comments',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Engineered Features back to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full Article Verbose / Write to file\n",
    "df_articles.to_csv('..//data//processed//df_articles_fe.csv')\n",
    "df_articles.to_pickle('..//data//processed//df_articles_fe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load exit from EDA\n",
    "df_articles = pd.read_pickle('..//data//processed//df_articles_fe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Entry point for ad-hoc analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_pickle('..//data//processed//df_articles_fe.pkl')\n",
    "\n",
    "### Drop poorly correlated columns -\n",
    "df_articles = df_articles.drop(columns=(df_articles.filter(like='LeadParagraph',axis=1).columns))\n",
    "df_articles = df_articles.drop(columns=(df_articles.filter(like='headline',axis=1).columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Profiles for EDA - Initial\n",
    "Create generic EDA, drill down where necesary\n",
    "\n",
    "EDA Ideas\n",
    "- Ratio of has n_comments to no n_comments\n",
    "- Missing data\n",
    "- Top Words\n",
    "- Top POS Speech\n",
    "- Correlations\n",
    "- Wordcloud\n",
    "- Comments by Month\n",
    "- Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_articles = ProfileReport(df_articles, title=\"New York Times - Articles \" + EDA_note)\n",
    "profile_articles.to_file('..//reports//figures//EDA_Articles' + EDA_note + '.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df_articles,x='n_comments',y='word_count');\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle('Word Count and n_comments', fontsize = 16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "heatmap = sns.heatmap(df_articles.corr(),vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Correlations', fontdict={'fontsize':18}, pad=16);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strongest correlations (still pretty weak) is the keywords and whether the article speaks of a person (TEXT_Keywords_ENT_PERSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df_articles,x='n_comments',y='TEXT_Keywords_ENT_PERSON')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle('Word Count and People Entities', fontsize = 16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count all Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for _ in df_articles['keywords'].apply(eval):\n",
    "    all_keywords.extend(_)\n",
    "cnt = Counter(all_keywords)\n",
    "# pp.pprint(cnt.most_common(10))\n",
    "\n",
    "df_count = pd.DataFrame.from_dict(cnt, orient='index').reset_index().sort_values(by=0,ascending=False)\n",
    "df_count = df_count.rename(columns={'index':'phrase', 0:'count'}).reset_index(drop=True)\n",
    "df_count\n",
    "\n",
    "cnt.most_common(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=10,10\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "df_count = df_count.head(10)\n",
    "p = sns.barplot(data=df_count,x='count',y='phrase')\n",
    "p.set_title('Most Common Keywords')\n",
    "df_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Sample Spacy NER visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(all_keywords[7])\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(all_keywords[7])\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Coronavirus (2019-nCoV)')\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "plt.rcParams['figure.figsize']=40,20\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "wordcloud = WordCloud().generate('|'.join(all_keywords))\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(10,8)\n",
    "hist = sns.histplot(data=df_articles,x='word_count', bins=30, kde=True)\n",
    "hist.set_title('Word Count Distribution', fontdict={'fontsize':14}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_Comments Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(10,8)\n",
    "hist2 = sns.histplot(data=df_articles,x='n_comments', bins=50,kde=True)\n",
    "hist2.set_title('N Comments Distribution', fontdict={'fontsize':14}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(10,8)\n",
    "hist2 = sns.histplot(data=df_articles,x='n_comments', bins=50,kde=True)\n",
    "hist2.set_title('N Comments Distribution', fontdict={'fontsize':14}, pad=16);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecaab7e4f2054dc1b3b08bfc690ea1c3d11e0c83265adf695ac1706ca10c6a57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
