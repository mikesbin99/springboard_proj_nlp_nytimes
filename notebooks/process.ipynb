{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_test_sample = pd.read_pickle('..//data//processed//df_test_sample.pkl')\n",
    "df_train_sample = pd.read_pickle('..//data//processed//df_train_sample.pkl')\n",
    "df_comments_sample = pd.read_pickle('..//data//processed//df_comments_sample.pkl')\n",
    "df_articles_sample = pd.read_pickle('..//data//processed//df_articles_sample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Canadian',\n",
       " 's',\n",
       " 'gruesome',\n",
       " 'account',\n",
       " 'as',\n",
       " 'an',\n",
       " 'Islamic',\n",
       " 'State',\n",
       " 'executioner',\n",
       " 'in',\n",
       " 'Syria',\n",
       " 'which',\n",
       " 'was',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Caliphate',\n",
       " 'podcast',\n",
       " 'by',\n",
       " 'The',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " 'was',\n",
       " 'fabricated',\n",
       " 'officials',\n",
       " 'say',\n",
       " 'A',\n",
       " 'Times',\n",
       " 'review',\n",
       " 'found',\n",
       " 'no',\n",
       " 'corroboration',\n",
       " 'of',\n",
       " 'his',\n",
       " 'claim',\n",
       " 'to',\n",
       " 'have',\n",
       " 'committed',\n",
       " 'atrocities']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = df_test_sample.iloc[6]['abstract']\n",
    "list(tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split into words and handling punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\AppData\\Local\\Temp/ipykernel_12892/1415527003.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = df_test_sample['abstract'].str.replace('[^a-zA-Z]',' ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1508    How four traumatic years turned Georgia into a...\n",
       "3873    On Jan     the vice president will preside as ...\n",
       "2875    One day  we ll look back on this year and bawl...\n",
       "2202    Have you seen more skateboarding parks in your...\n",
       "305     New York City had been holding off a second wa...\n",
       "                              ...                        \n",
       "1859    The  Chanel of streetwear  has a new owner  an...\n",
       "2432    The ownership of Taylor Swift s catalog return...\n",
       "992     Teenage comments in response to our recent wri...\n",
       "1116    Between a third and a half of all eligible vot...\n",
       "1369    Donald Trump has made America exceptional all ...\n",
       "Name: abstract, Length: 799, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df_test_sample['abstract'].str.replace('[^a-zA-Z]',' ')\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "remove the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def normalize_col(df, column, column_new):\n",
    "    #  https://towardsdatascience.com/text-cleaning-methods-\n",
    "    #  for-natural-language-processing-f2fc1796e8c7\n",
    "    df[column_new] = df[column].str.lower()\n",
    "    df[column_new] = df[column].apply\\\n",
    "        (lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|\\\n",
    "        (\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "\n",
    "    df[column_new] = df[column].apply\\\n",
    "        (lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_nrml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>How four traumatic years turned Georgia into a...</td>\n",
       "      <td>How four traumatic years turned Georgia into a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>On Jan. 6, the vice president will preside as ...</td>\n",
       "      <td>On Jan. , the vice president will preside as C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>One day, we’ll look back on this year and bawl...</td>\n",
       "      <td>One day, we’ll look back on this year and bawl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>Have you seen more skateboarding parks in your...</td>\n",
       "      <td>Have you seen more skateboarding parks in your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>New York City had been holding off a second wa...</td>\n",
       "      <td>New York City had been holding off a second wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>The “Chanel of streetwear” has a new owner, an...</td>\n",
       "      <td>The “Chanel of streetwear” has a new owner, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>The ownership of Taylor Swift’s catalog return...</td>\n",
       "      <td>The ownership of Taylor Swift’s catalog return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Teenage comments in response to our recent wri...</td>\n",
       "      <td>Teenage comments in response to our recent wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>Between a third and a half of all eligible vot...</td>\n",
       "      <td>Between a third and a half of all eligible vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>Donald Trump has made America exceptional all ...</td>\n",
       "      <td>Donald Trump has made America exceptional all ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "1508  How four traumatic years turned Georgia into a...   \n",
       "3873  On Jan. 6, the vice president will preside as ...   \n",
       "2875  One day, we’ll look back on this year and bawl...   \n",
       "2202  Have you seen more skateboarding parks in your...   \n",
       "305   New York City had been holding off a second wa...   \n",
       "...                                                 ...   \n",
       "1859  The “Chanel of streetwear” has a new owner, an...   \n",
       "2432  The ownership of Taylor Swift’s catalog return...   \n",
       "992   Teenage comments in response to our recent wri...   \n",
       "1116  Between a third and a half of all eligible vot...   \n",
       "1369  Donald Trump has made America exceptional all ...   \n",
       "\n",
       "                                          abstract_nrml  \n",
       "1508  How four traumatic years turned Georgia into a...  \n",
       "3873  On Jan. , the vice president will preside as C...  \n",
       "2875  One day, we’ll look back on this year and bawl...  \n",
       "2202  Have you seen more skateboarding parks in your...  \n",
       "305   New York City had been holding off a second wa...  \n",
       "...                                                 ...  \n",
       "1859  The “Chanel of streetwear” has a new owner, an...  \n",
       "2432  The ownership of Taylor Swift’s catalog return...  \n",
       "992   Teenage comments in response to our recent wri...  \n",
       "1116  Between a third and a half of all eligible vot...  \n",
       "1369  Donald Trump has made America exceptional all ...  \n",
       "\n",
       "[799 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = normalize_col(df_test_sample, 'abstract', 'abstract_nrml')\n",
    "data_clean[['abstract','abstract_nrml']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the stopwords\n",
    "stop[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, column, column_new):\n",
    "    df[column_new] = df[column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sample = remove_stopwords(df_test_sample, 'abstract_nrml','abstract_nrml_stp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_nrml</th>\n",
       "      <th>abstract_nrml_stp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>How four traumatic years turned Georgia into a...</td>\n",
       "      <td>How four traumatic years turned Georgia into a...</td>\n",
       "      <td>How four traumatic years turned Georgia swing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>On Jan. 6, the vice president will preside as ...</td>\n",
       "      <td>On Jan. , the vice president will preside as C...</td>\n",
       "      <td>On Jan. , vice president preside Congress coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>One day, we’ll look back on this year and bawl...</td>\n",
       "      <td>One day, we’ll look back on this year and bawl...</td>\n",
       "      <td>One day, we’ll look back year bawl. But also r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>Have you seen more skateboarding parks in your...</td>\n",
       "      <td>Have you seen more skateboarding parks in your...</td>\n",
       "      <td>Have seen skateboarding parks area? Do think s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>New York City had been holding off a second wa...</td>\n",
       "      <td>New York City had been holding off a second wa...</td>\n",
       "      <td>New York City holding second wave, uptick case...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>The “Chanel of streetwear” has a new owner, an...</td>\n",
       "      <td>The “Chanel of streetwear” has a new owner, an...</td>\n",
       "      <td>The “Chanel streetwear” new owner, tough balan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>The ownership of Taylor Swift’s catalog return...</td>\n",
       "      <td>The ownership of Taylor Swift’s catalog return...</td>\n",
       "      <td>The ownership Taylor Swift’s catalog returned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Teenage comments in response to our recent wri...</td>\n",
       "      <td>Teenage comments in response to our recent wri...</td>\n",
       "      <td>Teenage comments response recent writing promp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>Between a third and a half of all eligible vot...</td>\n",
       "      <td>Between a third and a half of all eligible vot...</td>\n",
       "      <td>Between third half eligible voters typically s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>Donald Trump has made America exceptional all ...</td>\n",
       "      <td>Donald Trump has made America exceptional all ...</td>\n",
       "      <td>Donald Trump made America exceptional right — ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "1508  How four traumatic years turned Georgia into a...   \n",
       "3873  On Jan. 6, the vice president will preside as ...   \n",
       "2875  One day, we’ll look back on this year and bawl...   \n",
       "2202  Have you seen more skateboarding parks in your...   \n",
       "305   New York City had been holding off a second wa...   \n",
       "...                                                 ...   \n",
       "1859  The “Chanel of streetwear” has a new owner, an...   \n",
       "2432  The ownership of Taylor Swift’s catalog return...   \n",
       "992   Teenage comments in response to our recent wri...   \n",
       "1116  Between a third and a half of all eligible vot...   \n",
       "1369  Donald Trump has made America exceptional all ...   \n",
       "\n",
       "                                          abstract_nrml  \\\n",
       "1508  How four traumatic years turned Georgia into a...   \n",
       "3873  On Jan. , the vice president will preside as C...   \n",
       "2875  One day, we’ll look back on this year and bawl...   \n",
       "2202  Have you seen more skateboarding parks in your...   \n",
       "305   New York City had been holding off a second wa...   \n",
       "...                                                 ...   \n",
       "1859  The “Chanel of streetwear” has a new owner, an...   \n",
       "2432  The ownership of Taylor Swift’s catalog return...   \n",
       "992   Teenage comments in response to our recent wri...   \n",
       "1116  Between a third and a half of all eligible vot...   \n",
       "1369  Donald Trump has made America exceptional all ...   \n",
       "\n",
       "                                      abstract_nrml_stp  \n",
       "1508  How four traumatic years turned Georgia swing ...  \n",
       "3873  On Jan. , vice president preside Congress coun...  \n",
       "2875  One day, we’ll look back year bawl. But also r...  \n",
       "2202  Have seen skateboarding parks area? Do think s...  \n",
       "305   New York City holding second wave, uptick case...  \n",
       "...                                                 ...  \n",
       "1859  The “Chanel streetwear” new owner, tough balan...  \n",
       "2432  The ownership Taylor Swift’s catalog returned ...  \n",
       "992   Teenage comments response recent writing promp...  \n",
       "1116  Between third half eligible voters typically s...  \n",
       "1369  Donald Trump made America exceptional right — ...  \n",
       "\n",
       "[799 rows x 3 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sample[['abstract','abstract_nrml','abstract_nrml_stp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def wrd_tokenize(df, column, column_new):\n",
    "    df[column_new] = df[column].apply(lambda x: word_tokenize(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test_sample = wrd_tokenize(df_test_sample, 'abstract_nrml_stp','abstract_nrml_stp_tokn')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test_sample.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test_sample['abstract_nrml_stp_tokn']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def wrd_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n",
    "    # new_col_name = column + '_wrd_stemmer'\n",
    "    # df[new_col_name] = df[column].apply(lambda x: word_stemmer(x))\n",
    "    # return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test_sample['abstract_clean_wrd_tokenized_stemmed'] = df_test_sample['abstract_clean_wrd_tokenized'].\\\n",
    "    apply(lambda x: wrd_stemmer(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test_sample['abstract_clean_wrd_tokenized_stemmed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Make words in root form, morphological analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrd_lemmatize(text):\n",
    "    lemmaed = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lemmaed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['newsdesk', 'section', 'subsection', 'material', 'headline', 'abstract',\n",
       "       'keywords', 'word_count', 'pub_date', 'is_popular', 'uniqueID',\n",
       "       'abstract_nrml', 'abstact_nrml_stp', 'abstract_nrml_stp',\n",
       "       'abstract_nrml_stp_tokn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sample['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sample['text_tokens_lemmaed'] = df_test_sample['abstract_nrml_stp'] \\\n",
    "                    .apply(lambda x: wrd_lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1508    [H, o, w,  , f, o, u, r,  , t, r, a, u, m, a, ...\n",
       "3873    [O, n,  , J, a, n, .,  , ,,  , v, i, c, e,  , ...\n",
       "2875    [O, n, e,  , d, a, y, ,,  , w, e, ’, l, l,  , ...\n",
       "2202    [H, a, v, e,  , s, e, e, n,  , s, k, a, t, e, ...\n",
       "305     [N, e, w,  , Y, o, r, k,  , C, i, t, y,  , h, ...\n",
       "                              ...                        \n",
       "1859    [T, h, e,  , “, C, h, a, n, e, l,  , s, t, r, ...\n",
       "2432    [T, h, e,  , o, w, n, e, r, s, h, i, p,  , T, ...\n",
       "992     [T, e, e, n, a, g, e,  , c, o, m, m, e, n, t, ...\n",
       "1116    [B, e, t, w, e, e, n,  , t, h, i, r, d,  , h, ...\n",
       "1369    [D, o, n, a, l, d,  , T, r, u, m, p,  , m, a, ...\n",
       "Name: text_tokens_lemmaed, Length: 799, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sample['text_tokens_lemmaed'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecaab7e4f2054dc1b3b08bfc690ea1c3d11e0c83265adf695ac1706ca10c6a57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
