{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pprint as pp\n",
    "\n",
    "\n",
    "# displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "nlp = spacy.load('en_core_web_md') #Language Class Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_pickle('..//data//processed//df_articles.pkl')\n",
    "\n",
    "# # Convert Timestamps where necessary\n",
    "# df_articles['pub_date'] = pd.to_datetime(df_articles['pub_date'])\n",
    "\n",
    "# # Drop constant columns, userTitle is 99% empty\n",
    "# df_articles.drop(columns=['uri','byline.person'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">['\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States International Relations'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", '\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States Politics and Government'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", 'Targeted Killings', \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    'Republican Party'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", '\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lovecraft, H P'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Suleimani\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Qassim', '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donald J'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "]</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entity Recognizer\n",
    "doc = nlp(df_articles.iloc[300]['keywords']) # Make a Doc object\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Much of the work of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    H.P. Lovecraft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " horror and science fiction writer who worked during \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the first decades of the 20th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", is defined by individual encounters with the incomprehensible, with sights, sounds and ideas that undermine and disturb reality as his characters understand it. Faced with things too monstrous to be real, but which exist nonetheless, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lovecraftian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " protagonists either reject their senses or descend into madness, unable to live with what they’ve learned.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entity Recognizer\n",
    "doc = nlp(df_articles.iloc[300]['lead_paragraph'])\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.dropna(subset=['keywords'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['clean_keywords'] = df_articles['keywords'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', None, pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "    return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "'\n",
      "Kushner\n",
      ",\n",
      "Jared\n",
      "'\n",
      ",\n",
      "'\n",
      "Presidential\n",
      "Election\n",
      "of\n",
      "2020\n",
      "'\n",
      ",\n",
      "'\n",
      "United\n",
      "States\n",
      "International\n",
      "Relations\n",
      "'\n",
      ",\n",
      "'\n",
      "Trump\n",
      ",\n",
      "Donald\n",
      "J\n",
      "'\n",
      ",\n",
      "'\n",
      "Trump\n",
      ",\n",
      "Ivanka\n",
      "'\n",
      ",\n",
      "'\n",
      "United\n",
      "States\n",
      "Politics\n",
      "and\n",
      "Government\n",
      "'\n",
      ",\n",
      "'\n",
      "Iran\n",
      "'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(df_articles.iloc[400]['keywords'])\n",
    "# # print(extract_full_name(doc))\n",
    "# for token in doc:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many sentences do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much of the work of H.P. Lovecraft, an American horror and science fiction writer who worked during the first decades of the 20th century, is defined by individual encounters with the incomprehensible, with sights, sounds and ideas that undermine and disturb reality as his characters understand it.\n",
      "Faced with things too monstrous to be real, but which exist nonetheless, Lovecraftian protagonists either reject their senses or descend into madness, unable to live with what they’ve learned.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "text = nlp(df_articles.iloc[300]['lead_paragraph'])\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "\n",
    "print(len(list(doc.sents)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much much\n",
      "of of\n",
      "the the\n",
      "work work\n",
      "of of\n",
      "H.P. H.P.\n",
      "Lovecraft Lovecraft\n",
      ", ,\n",
      "an an\n",
      "American american\n",
      "horror horror\n",
      "and and\n",
      "science science\n",
      "fiction fiction\n",
      "writer writer\n",
      "who who\n",
      "worked work\n",
      "during during\n",
      "the the\n",
      "first first\n",
      "decades decade\n",
      "of of\n",
      "the the\n",
      "20th 20th\n",
      "century century\n",
      ", ,\n",
      "is be\n",
      "defined define\n",
      "by by\n",
      "individual individual\n",
      "encounters encounter\n",
      "with with\n",
      "the the\n",
      "incomprehensible incomprehensible\n",
      ", ,\n",
      "with with\n",
      "sights sight\n",
      ", ,\n",
      "sounds sound\n",
      "and and\n",
      "ideas idea\n",
      "that that\n",
      "undermine undermine\n",
      "and and\n",
      "disturb disturb\n",
      "reality reality\n",
      "as as\n",
      "his his\n",
      "characters character\n",
      "understand understand\n",
      "it it\n",
      ". .\n",
      "Faced face\n",
      "with with\n",
      "things thing\n",
      "too too\n",
      "monstrous monstrous\n",
      "to to\n",
      "be be\n",
      "real real\n",
      ", ,\n",
      "but but\n",
      "which which\n",
      "exist exist\n",
      "nonetheless nonetheless\n",
      ", ,\n",
      "Lovecraftian Lovecraftian\n",
      "protagonists protagonist\n",
      "either either\n",
      "reject reject\n",
      "their their\n",
      "senses sense\n",
      "or or\n",
      "descend descend\n",
      "into into\n",
      "madness madness\n",
      ", ,\n",
      "unable unable\n",
      "to to\n",
      "live live\n",
      "with with\n",
      "what what\n",
      "they they\n",
      "’ve ’ve\n",
      "learned learn\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[300]['lead_paragraph'])\n",
    "# doc = nlp(df_articles.iloc[300]['keywords'])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(H.P. Lovecraft, American, the first decades of the 20th century, Lovecraftian)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noun Phrases\n",
    "\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[the work,\n",
       " H.P. Lovecraft,\n",
       " an American horror and science fiction writer,\n",
       " who,\n",
       " the first decades,\n",
       " the 20th century,\n",
       " individual encounters,\n",
       " the incomprehensible,\n",
       " sights,\n",
       " sounds,\n",
       " ideas,\n",
       " that,\n",
       " reality,\n",
       " his characters,\n",
       " it,\n",
       " things,\n",
       " which,\n",
       " their senses,\n",
       " madness,\n",
       " what,\n",
       " they]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is the POS breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Counter.most_common of Counter({'PUNCT': 33, 'PROPN': 18, 'NOUN': 7, 'PART': 5, 'CCONJ': 4})>\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[700]['lead_paragraph'])\n",
    "doc = nlp(df_articles.iloc[700]['keywords'])\n",
    "\n",
    "pos = []\n",
    "\n",
    "for token in doc:\n",
    "    pos.append(token.pos_)\n",
    "#     if token.pos_ == 'NOUN':\n",
    "#         print(token.pos_,token.text)\n",
    "\n",
    "# for token in doc:\n",
    "#     if token.pos_ == 'VERB':\n",
    "#         print(token.pos_,token.text)\n",
    "\n",
    "pos_ctr = Counter(pos)\n",
    "pp.pprint(pos_ctr.most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Mississippi, 1),\n",
       " (Prison Guards and Corrections Officers', 1),\n",
       " (Criminal Justice', 1),\n",
       " (Mississippi State Penitentiary', 1),\n",
       " (Homicides, 1)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = Counter(doc.ents)\n",
    "ctr.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Counter.most_common of Counter({'PUNCT': 7, 'PROPN': 6, 'ADP': 5, 'DET': 4, 'NOUN': 3, 'VERB': 3, 'PRON': 2, 'ADV': 1, 'SCONJ': 1, 'ADJ': 1, 'AUX': 1, 'NUM': 1, 'INTJ': 1})>\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[700]['lead_paragraph'])\n",
    "# doc = nlp(df_articles.iloc[700]['keywords'])\n",
    "\n",
    "pos = []\n",
    "\n",
    "for token in doc:\n",
    "    pos.append(token.pos_)\n",
    "#     if token.pos_ == 'NOUN':\n",
    "#         print(token.pos_,token.text)\n",
    "\n",
    "# for token in doc:\n",
    "#     if token.pos_ == 'VERB':\n",
    "#         print(token.pos_,token.text)\n",
    "\n",
    "# set(pos)\n",
    "pos_ctr = Counter(pos)\n",
    "pp.pprint(pos_ctr.most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[700]['lead_paragraph'])\n",
    "pos = []\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    pos.append(token.pos_)\n",
    "\n",
    "pos = Counter(pos)\n",
    "pos['VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['PROPN', 'PUNCT', 'DET', 'NOUN', 'VERB', 'ADV', 'SCONJ', 'PRON', 'ADP', 'ADJ', 'AUX', 'NUM', 'INTJ'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ctr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ctr['NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \n",
      "' \n",
      "Mississippi GPE\n",
      "' \n",
      ", \n",
      "' \n",
      "Prisons \n",
      "and \n",
      "Prisoners \n",
      "' \n",
      ", \n",
      "' \n",
      "Cellular \n",
      "Telephones \n",
      "' \n",
      ", \n",
      "' \n",
      "Smuggling \n",
      "' \n",
      ", \n",
      "' \n",
      "Prison ORG\n",
      "Guards ORG\n",
      "and ORG\n",
      "Corrections ORG\n",
      "Officers ORG\n",
      "' ORG\n",
      ", \n",
      "' \n",
      "Criminal ORG\n",
      "Justice ORG\n",
      "' ORG\n",
      ", \n",
      "' \n",
      "Mississippi ORG\n",
      "State ORG\n",
      "Penitentiary ORG\n",
      "' ORG\n",
      ", \n",
      "' \n",
      "Parchman \n",
      "( \n",
      "Miss \n",
      ") \n",
      "' \n",
      ", \n",
      "' \n",
      "Murders \n",
      ", \n",
      "Attempted \n",
      "Murders \n",
      "and \n",
      "Homicides PERSON\n",
      "' \n",
      ", \n",
      "' \n",
      "Assaults \n",
      "' \n",
      ", \n",
      "' \n",
      "Demonstrations \n",
      ", \n",
      "Protests \n",
      "and \n",
      "Riots \n",
      "' \n",
      "] \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[700]['keywords'])\n",
    "# doc = nlp(df_articles.iloc[700]['keywords'])\n",
    "\n",
    "doc.ents\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.ents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">['Military Tribunals', '\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Guantanamo Bay Naval Base\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cuba\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ")', 'Interrogations', 'Torture', '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mitchell\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    James E'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jessen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bruce'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", '\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Central Intelligence Agency'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", '\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Federal Bureau of Investigation'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", 'Terrorism', 'September 11 (\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2001\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ")', '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mohammed\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Khalid Shaikh'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", '\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States Defense and Military Forces'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", 'Detainees']</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[900]['keywords']) # Make a Doc object\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Military Forces'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(df_articles.iloc[900]['keywords'])\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "   # Get only names from here\n",
    "\n",
    "   pattern = [{'POS': 'PROPN'}, \n",
    "              {'POS': 'PROPN'}]\n",
    "\n",
    "   matcher.add('FULL_NAME', [pattern])\n",
    "   matches = matcher(nlp_doc)\n",
    "   for match_id, start, end in matches:\n",
    "      span = nlp_doc[start:end]\n",
    "\n",
    "   return span.text\n",
    "\n",
    "extract_full_name(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_20368/1283036612.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Mike\\AppData\\Local\\Temp/ipykernel_20368/1283036612.py\"\u001b[1;36m, line \u001b[1;32m30\u001b[0m\n\u001b[1;33m    nlp.add_pipe(\"expand_person_entities\", after='ner')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "import dateparser\n",
    "\n",
    "# https://stackoverflow.com/questions/51490620/extracting-names-from-a-text-file-using-spacy\n",
    "\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check for title if it's a person and not the first token\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.start != 0:\n",
    "                # if person preceded by title, include title in entity\n",
    "                prev_token = doc[ent.start - 1]\n",
    "                if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                    new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                    new_ents.append(new_ent)\n",
    "                else:\n",
    "                    # if entity can be parsed as a date, it's not a person\n",
    "                    if dateparser.parse(ent.text) is None:\n",
    "                        new_ents.append(ent) \n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "# Add the component after the named entity recognizer\n",
    "# nlp.remove_pipe('expand_person_entities')\n",
    "\n",
    "@Language.component('expand_person_entities')\n",
    "nlp.add_pipe(\"expand_person_entities\", after='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     2\n",
      "2     3\n",
      "3     4\n",
      "4     5\n",
      "5    89\n",
      "Name: numbers, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3,4,5,89]\n",
    "\n",
    "ls = pd.Series(l,name='numbers')\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "    'lemmas'          : extract_lemmas(doc,\n",
    "                                        exclude_pos = ['PART', 'PUNCT',\n",
    "                                        'DET', 'PRON', 'SYM', 'SPACE'],\n",
    "                                        filter_stops = False),\n",
    "    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
    "    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "    'noun_phrases'    : extract_noun_phrases(doc, ['NOUN']),\n",
    "    'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n",
    "    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spacy Find Names\n",
    "def extract_full_name(nlp_doc):\n",
    "    matcher.add('FULL_NAME', [{'POS': 'PROPN'}, {'POS': 'PROPN'}])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:      \n",
    "        span = nlp_doc[start:end]\n",
    "    return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E178] Each pattern should be a list of dicts, but got: {'POS': 'PROPN'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('FULL_NAME', [pattern])`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20368/2959634754.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mextract_full_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20368/3897368127.py\u001b[0m in \u001b[0;36mextract_full_name\u001b[1;34m(nlp_doc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Spacy Find Names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_full_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FULL_NAME'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'POS'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'PROPN'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'POS'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'PROPN'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\spacy\\matcher\\matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E178] Each pattern should be a list of dicts, but got: {'POS': 'PROPN'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('FULL_NAME', [pattern])`"
     ]
    }
   ],
   "source": [
    "extract_full_name(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'merge_entities' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'merge_entities']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20368/1491241532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"merge_entities\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_articles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lead_paragraph'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m pattern = [{'POS': 'PROPN'}, \n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m             \u001b[1;31m# We're loading the component from a model. After loading the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E007] 'merge_entities' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'merge_entities']"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "\n",
    "doc = nlp(df_articles.iloc[300]['lead_paragraph'])\n",
    "\n",
    "pattern = [{'POS': 'PROPN'}, \n",
    "              {'POS': 'PROPN'}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('person_only', [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">['Traffic Accidents and Safety', 'Bicycles and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bicycling\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "', 'Drunken and Reckless Driving', 'Deaths (Fatalities)', 'Accidents and Safety', 'Roads and Traffic', 'de Blasio, Bill', '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Johnson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Corey\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "', '\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trottenberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Polly', '\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York City'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "]</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[20]['keywords'])\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "MatchPatternError",
     "evalue": "Invalid token patterns for matcher rule 'person_only'\n\nPattern 0:\n- [pattern -> 0 -> ENT] extra fields not permitted\n- [pattern -> 1 -> ENT] extra fields not permitted\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMatchPatternError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20368/2489022132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'person_only'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\spacy\\matcher\\matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMatchPatternError\u001b[0m: Invalid token patterns for matcher rule 'person_only'\n\nPattern 0:\n- [pattern -> 0 -> ENT] extra fields not permitted\n- [pattern -> 1 -> ENT] extra fields not permitted\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df_articles.iloc[20]['keywords'])\n",
    "\n",
    "pattern = [{'POS': 'PROPN'}, \n",
    "{'POS': 'PROPN'}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('person_only', [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecaab7e4f2054dc1b3b08bfc690ea1c3d11e0c83265adf695ac1706ca10c6a57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
