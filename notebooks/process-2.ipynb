{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:\\My Drive\\Code\\springboard_proj_nlp_nytimes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Project Root\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "print(os.path.abspath(PROJ_ROOT))\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# import my method from the source code\n",
    "# from features.build_features import remove_invalid_data\n",
    "# from features.build_features import awesome_function\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as plty\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import html\n",
    "import re\n",
    "\n",
    "# Textacy\n",
    "import textacy.preprocessing as tprep # Preprocesing of accents/normalization\n",
    "from textacy.preprocessing.resources import RE_URL\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Set to Reload all custom packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# Custom developed tools\n",
    "import nlp_tools\n",
    "import constants\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_test = pd.read_pickle('..//data//processed//df_test_sample.pkl')\n",
    "df_train = pd.read_pickle('..//data//processed//df_train_sample.pkl')\n",
    "df_comments = pd.read_pickle('..//data//processed//df_comments_sample.pkl')\n",
    "df_articles = pd.read_pickle('..//data//processed//df_articles_sample.pkl')\n",
    "\n",
    "# # Load Data\n",
    "# df_test = pd.read_pickle('..//data//raw//df_test.pkl')\n",
    "# df_train = pd.read_pickle('..//data//raw//df_train.pkl')\n",
    "# df_comments = pd.read_pickle('..//data//raw//df_comments.pkl')\n",
    "# df_articles = pd.read_pickle('..//data//raw//df_articles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2558 entries, 8527 to 10328\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   newsdesk    2558 non-null   object\n",
      " 1   section     2558 non-null   object\n",
      " 2   subsection  883 non-null    object\n",
      " 3   material    2558 non-null   object\n",
      " 4   headline    2558 non-null   object\n",
      " 5   abstract    2557 non-null   object\n",
      " 6   keywords    2558 non-null   object\n",
      " 7   word_count  2558 non-null   int64 \n",
      " 8   pub_date    2558 non-null   object\n",
      " 9   is_popular  2558 non-null   int64 \n",
      " 10  n_comments  2558 non-null   int64 \n",
      " 11  uniqueID    2558 non-null   object\n",
      "dtypes: int64(3), object(9)\n",
      "memory usage: 259.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsdesk</th>\n",
       "      <th>section</th>\n",
       "      <th>subsection</th>\n",
       "      <th>material</th>\n",
       "      <th>headline</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>word_count</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>uniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>Foreign</td>\n",
       "      <td>World</td>\n",
       "      <td>Europe</td>\n",
       "      <td>News</td>\n",
       "      <td>France Brings 10 Children of French Jihadists ...</td>\n",
       "      <td>About 270 children of French citizens remain s...</td>\n",
       "      <td>['France', 'Islamic State in Iraq and Syria (I...</td>\n",
       "      <td>1131</td>\n",
       "      <td>2020-06-22 21:20:53+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>nyt://article/f1e6b417-f8ca-597d-9be9-db2ca917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Politics</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>Politics</td>\n",
       "      <td>News</td>\n",
       "      <td>How Elizabeth Warren Is Being Squeezed by 2 De...</td>\n",
       "      <td>Ms. Warren is figuring out what candidates lik...</td>\n",
       "      <td>['Presidential Election of 2020', 'Warren, Eli...</td>\n",
       "      <td>1442</td>\n",
       "      <td>2020-01-08 19:12:09+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "      <td>nyt://article/88600e52-644c-56c3-805f-af931533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>Styles</td>\n",
       "      <td>Style</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News</td>\n",
       "      <td>They Fled the Coronavirus. Now They’re in Scen...</td>\n",
       "      <td>Some feel sheepish about their choice to retre...</td>\n",
       "      <td>['Travel and Vacations', 'Coronavirus (2019-nC...</td>\n",
       "      <td>1989</td>\n",
       "      <td>2020-05-11 09:00:20+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>362</td>\n",
       "      <td>nyt://article/fd7d10a8-9bc0-55b2-8829-0ac8b275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>Learning</td>\n",
       "      <td>The Learning Network</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News</td>\n",
       "      <td>How Do You Greet Your Friends and Family?</td>\n",
       "      <td>Do you hug friends or use handshakes? How have...</td>\n",
       "      <td>[]</td>\n",
       "      <td>648</td>\n",
       "      <td>2020-04-23 12:19:17+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>nyt://article/2554b892-d3b3-505e-8cc8-d1ddbf5a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10337</th>\n",
       "      <td>Foreign</td>\n",
       "      <td>World</td>\n",
       "      <td>Australia</td>\n",
       "      <td>News</td>\n",
       "      <td>What Lockdown 2.0 Looks Like: Harsher Rules, D...</td>\n",
       "      <td>Melbourne, Australia’s second-largest city, is...</td>\n",
       "      <td>['Quarantines', 'Coronavirus (2019-nCoV)', 'Au...</td>\n",
       "      <td>1505</td>\n",
       "      <td>2020-08-04 09:55:38+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>294</td>\n",
       "      <td>nyt://article/ba22d655-75b2-57ca-8822-3b071e7c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       newsdesk               section subsection material  \\\n",
       "8527    Foreign                 World     Europe     News   \n",
       "322    Politics                  U.S.   Politics     News   \n",
       "6565     Styles                 Style        NaN     News   \n",
       "5655   Learning  The Learning Network        NaN     News   \n",
       "10337   Foreign                 World  Australia     News   \n",
       "\n",
       "                                                headline  \\\n",
       "8527   France Brings 10 Children of French Jihadists ...   \n",
       "322    How Elizabeth Warren Is Being Squeezed by 2 De...   \n",
       "6565   They Fled the Coronavirus. Now They’re in Scen...   \n",
       "5655           How Do You Greet Your Friends and Family?   \n",
       "10337  What Lockdown 2.0 Looks Like: Harsher Rules, D...   \n",
       "\n",
       "                                                abstract  \\\n",
       "8527   About 270 children of French citizens remain s...   \n",
       "322    Ms. Warren is figuring out what candidates lik...   \n",
       "6565   Some feel sheepish about their choice to retre...   \n",
       "5655   Do you hug friends or use handshakes? How have...   \n",
       "10337  Melbourne, Australia’s second-largest city, is...   \n",
       "\n",
       "                                                keywords  word_count  \\\n",
       "8527   ['France', 'Islamic State in Iraq and Syria (I...        1131   \n",
       "322    ['Presidential Election of 2020', 'Warren, Eli...        1442   \n",
       "6565   ['Travel and Vacations', 'Coronavirus (2019-nC...        1989   \n",
       "5655                                                  []         648   \n",
       "10337  ['Quarantines', 'Coronavirus (2019-nCoV)', 'Au...        1505   \n",
       "\n",
       "                        pub_date  is_popular  n_comments  \\\n",
       "8527   2020-06-22 21:20:53+00:00           0           6   \n",
       "322    2020-01-08 19:12:09+00:00           1         397   \n",
       "6565   2020-05-11 09:00:20+00:00           1         362   \n",
       "5655   2020-04-23 12:19:17+00:00           1         168   \n",
       "10337  2020-08-04 09:55:38+00:00           1         294   \n",
       "\n",
       "                                                uniqueID  \n",
       "8527   nyt://article/f1e6b417-f8ca-597d-9be9-db2ca917...  \n",
       "322    nyt://article/88600e52-644c-56c3-805f-af931533...  \n",
       "6565   nyt://article/fd7d10a8-9bc0-55b2-8829-0ac8b275...  \n",
       "5655   nyt://article/2554b892-d3b3-505e-8cc8-d1ddbf5a...  \n",
       "10337  nyt://article/ba22d655-75b2-57ca-8822-3b071e7c...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:00:10+00:00</th>\n",
       "      <td>931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 05:00:12+00:00</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 10:00:01+00:00</th>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 10:01:24+00:00</th>\n",
       "      <td>1403.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 13:00:09+00:00</th>\n",
       "      <td>1957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 09:01:54+00:00</th>\n",
       "      <td>1873.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 11:18:25+00:00</th>\n",
       "      <td>923.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 14:00:13+00:00</th>\n",
       "      <td>1983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 16:37:58+00:00</th>\n",
       "      <td>1142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30 23:44:30+00:00</th>\n",
       "      <td>1580.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2518 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           word_count\n",
       "pub_date                             \n",
       "2020-01-01 03:00:10+00:00       931.0\n",
       "2020-01-01 05:00:12+00:00         0.0\n",
       "2020-01-01 10:00:01+00:00       153.0\n",
       "2020-01-01 10:01:24+00:00      1403.0\n",
       "2020-01-01 13:00:09+00:00      1957.0\n",
       "...                               ...\n",
       "2020-09-30 09:01:54+00:00      1873.0\n",
       "2020-09-30 11:18:25+00:00       923.0\n",
       "2020-09-30 14:00:13+00:00      1983.0\n",
       "2020-09-30 16:37:58+00:00      1142.0\n",
       "2020-09-30 23:44:30+00:00      1580.0\n",
       "\n",
       "[2518 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('pub_date').agg({'word_count':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 839 entries, 15829 to 11825\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   newsdesk    839 non-null    object\n",
      " 1   section     839 non-null    object\n",
      " 2   subsection  308 non-null    object\n",
      " 3   material    839 non-null    object\n",
      " 4   headline    839 non-null    object\n",
      " 5   abstract    839 non-null    object\n",
      " 6   keywords    839 non-null    object\n",
      " 7   word_count  839 non-null    int64 \n",
      " 8   pub_date    839 non-null    object\n",
      " 9   n_comments  839 non-null    int64 \n",
      " 10  uniqueID    839 non-null    object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 78.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49865 entries, 2893132 to 463789\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   commentID              49865 non-null  int64  \n",
      " 1   status                 49865 non-null  object \n",
      " 2   commentSequence        49865 non-null  int64  \n",
      " 3   userID                 49865 non-null  int64  \n",
      " 4   userDisplayName        49849 non-null  object \n",
      " 5   userLocation           49845 non-null  object \n",
      " 6   userTitle              42 non-null     object \n",
      " 7   commentBody            49865 non-null  object \n",
      " 8   createDate             49865 non-null  object \n",
      " 9   updateDate             49865 non-null  object \n",
      " 10  approveDate            49865 non-null  object \n",
      " 11  recommendations        49865 non-null  int64  \n",
      " 12  replyCount             49865 non-null  int64  \n",
      " 13  editorsSelection       49865 non-null  bool   \n",
      " 14  parentID               21575 non-null  float64\n",
      " 15  parentUserDisplayName  21566 non-null  object \n",
      " 16  depth                  49865 non-null  int64  \n",
      " 17  commentType            49865 non-null  object \n",
      " 18  trusted                49865 non-null  int64  \n",
      " 19  recommendedFlag        49865 non-null  int64  \n",
      " 20  permID                 49865 non-null  int64  \n",
      " 21  isAnonymous            49865 non-null  bool   \n",
      " 22  articleID              49865 non-null  object \n",
      "dtypes: bool(2), float64(1), int64(9), object(11)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>newsdesk</th>\n",
       "      <td>839</td>\n",
       "      <td>42</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <td>839</td>\n",
       "      <td>32</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsection</th>\n",
       "      <td>308</td>\n",
       "      <td>37</td>\n",
       "      <td>Politics</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>material</th>\n",
       "      <td>839</td>\n",
       "      <td>8</td>\n",
       "      <td>News</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headline</th>\n",
       "      <td>839</td>\n",
       "      <td>835</td>\n",
       "      <td>Coronavirus in N.Y.C.: Latest Updates</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstract</th>\n",
       "      <td>839</td>\n",
       "      <td>832</td>\n",
       "      <td>Recent residential sales in New York City and ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keywords</th>\n",
       "      <td>839</td>\n",
       "      <td>780</td>\n",
       "      <td>[]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_date</th>\n",
       "      <td>839</td>\n",
       "      <td>834</td>\n",
       "      <td>2020-02-25 10:00:22+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniqueID</th>\n",
       "      <td>839</td>\n",
       "      <td>839</td>\n",
       "      <td>nyt://article/95f224dd-f15f-5f98-ba28-159c3107...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count unique                                                top  \\\n",
       "newsdesk     839     42                                               OpEd   \n",
       "section      839     32                                               U.S.   \n",
       "subsection   308     37                                           Politics   \n",
       "material     839      8                                               News   \n",
       "headline     839    835              Coronavirus in N.Y.C.: Latest Updates   \n",
       "abstract     839    832  Recent residential sales in New York City and ...   \n",
       "keywords     839    780                                                 []   \n",
       "pub_date     839    834                          2020-02-25 10:00:22+00:00   \n",
       "uniqueID     839    839  nyt://article/95f224dd-f15f-5f98-ba28-159c3107...   \n",
       "\n",
       "           freq  \n",
       "newsdesk     89  \n",
       "section     128  \n",
       "subsection   77  \n",
       "material    657  \n",
       "headline      3  \n",
       "abstract      3  \n",
       "keywords     34  \n",
       "pub_date      2  \n",
       "uniqueID      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.describe(include='O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# newsgroups_data = fetch_20newsgroups()\n",
    "# print(newsgroups_data.data[500])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <td>49865</td>\n",
       "      <td>1</td>\n",
       "      <td>approved</td>\n",
       "      <td>49865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userDisplayName</th>\n",
       "      <td>49849</td>\n",
       "      <td>20041</td>\n",
       "      <td>John</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userLocation</th>\n",
       "      <td>49845</td>\n",
       "      <td>8257</td>\n",
       "      <td>NYC</td>\n",
       "      <td>1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userTitle</th>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commentBody</th>\n",
       "      <td>49865</td>\n",
       "      <td>49849</td>\n",
       "      <td>Thank you.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createDate</th>\n",
       "      <td>49865</td>\n",
       "      <td>49803</td>\n",
       "      <td>2020-02-16 16:51:10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>updateDate</th>\n",
       "      <td>49865</td>\n",
       "      <td>49793</td>\n",
       "      <td>2020-10-28 15:09:11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approveDate</th>\n",
       "      <td>49865</td>\n",
       "      <td>49237</td>\n",
       "      <td>2020-04-09 15:39:56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parentUserDisplayName</th>\n",
       "      <td>21566</td>\n",
       "      <td>10037</td>\n",
       "      <td>Doug</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commentType</th>\n",
       "      <td>49865</td>\n",
       "      <td>3</td>\n",
       "      <td>comment</td>\n",
       "      <td>28289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>articleID</th>\n",
       "      <td>49865</td>\n",
       "      <td>9448</td>\n",
       "      <td>nyt://interactive/a36f9b6d-eea3-5c5c-be2d-ee90...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count unique  \\\n",
       "status                 49865      1   \n",
       "userDisplayName        49849  20041   \n",
       "userLocation           49845   8257   \n",
       "userTitle                 42     12   \n",
       "commentBody            49865  49849   \n",
       "createDate             49865  49803   \n",
       "updateDate             49865  49793   \n",
       "approveDate            49865  49237   \n",
       "parentUserDisplayName  21566  10037   \n",
       "commentType            49865      3   \n",
       "articleID              49865   9448   \n",
       "\n",
       "                                                                     top  \\\n",
       "status                                                          approved   \n",
       "userDisplayName                                                     John   \n",
       "userLocation                                                         NYC   \n",
       "userTitle                                             The New York Times   \n",
       "commentBody                                                   Thank you.   \n",
       "createDate                                           2020-02-16 16:51:10   \n",
       "updateDate                                           2020-10-28 15:09:11   \n",
       "approveDate                                          2020-04-09 15:39:56   \n",
       "parentUserDisplayName                                               Doug   \n",
       "commentType                                                      comment   \n",
       "articleID              nyt://interactive/a36f9b6d-eea3-5c5c-be2d-ee90...   \n",
       "\n",
       "                        freq  \n",
       "status                 49865  \n",
       "userDisplayName          267  \n",
       "userLocation            1680  \n",
       "userTitle                 14  \n",
       "commentBody                4  \n",
       "createDate                 2  \n",
       "updateDate                 2  \n",
       "approveDate                3  \n",
       "parentUserDisplayName    147  \n",
       "commentType            28289  \n",
       "articleID                 87  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean escapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_tools\n",
    "df_comments['commentClean'] = df_comments['commentBody'].apply(nlp_tools.clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize / Split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comments['commentTokens'] = df_comments['commentClean'].apply(nlp_tools.tokenizeText)\n",
    "# df_comments.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['commentLemmas'] = df_comments['commentTokens'].apply(nlp_tools.lemmatizeTokenList)\n",
    "# print(df_comments.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['commentNoStopWords'] = df_comments['commentLemmas'].apply(nlp_tools.cleanStopWords)\n",
    "# print(df_comments['commentNoStopWords'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentClean</th>\n",
       "      <th>commentTokens</th>\n",
       "      <th>commentLemmas</th>\n",
       "      <th>commentNoStopWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893132</th>\n",
       "      <td>@Jo Williams Exactly I see a nightmarish Gotha...</td>\n",
       "      <td>[@Jo, Williams, Exactly, I, see, a, nightmaris...</td>\n",
       "      <td>[@jo, williams, exactly, i, see, a, nightmaris...</td>\n",
       "      <td>[@jo, williams, exactly, see, nightmarish, got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599853</th>\n",
       "      <td>@Bugmon Are you really that obtuse? I do wear ...</td>\n",
       "      <td>[@Bugmon, Are, you, really, that, obtuse, ?, I...</td>\n",
       "      <td>[@bugmon, be, you, really, that, obtuse, ?, i,...</td>\n",
       "      <td>[@bugmon, really, obtuse, ?, wear, mask, go, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125068</th>\n",
       "      <td>It's so sad to see Trump's impact on our natio...</td>\n",
       "      <td>[It, 's, so, sad, to, see, Trump, 's, impact, ...</td>\n",
       "      <td>[it, 's, so, sad, to, see, trump, 's, impact, ...</td>\n",
       "      <td>['s, sad, see, trump, 's, impact, nation, ., w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425848</th>\n",
       "      <td>The children’s hospital in my city has enormou...</td>\n",
       "      <td>[The, children, ’s, hospital, in, my, city, ha...</td>\n",
       "      <td>[the, children, ’s, hospital, in, my, city, ha...</td>\n",
       "      <td>[children, ’s, hospital, city, enormous, cash,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444275</th>\n",
       "      <td>I can't wait to watch this virtually with othe...</td>\n",
       "      <td>[I, can, 't, wait, to, watch, this, virtually,...</td>\n",
       "      <td>[i, can, 't, wait, to, watch, this, virtually,...</td>\n",
       "      <td>['t, wait, watch, virtually, folks, ., grandmo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              commentClean  \\\n",
       "2893132  @Jo Williams Exactly I see a nightmarish Gotha...   \n",
       "2599853  @Bugmon Are you really that obtuse? I do wear ...   \n",
       "125068   It's so sad to see Trump's impact on our natio...   \n",
       "2425848  The children’s hospital in my city has enormou...   \n",
       "1444275  I can't wait to watch this virtually with othe...   \n",
       "\n",
       "                                             commentTokens  \\\n",
       "2893132  [@Jo, Williams, Exactly, I, see, a, nightmaris...   \n",
       "2599853  [@Bugmon, Are, you, really, that, obtuse, ?, I...   \n",
       "125068   [It, 's, so, sad, to, see, Trump, 's, impact, ...   \n",
       "2425848  [The, children, ’s, hospital, in, my, city, ha...   \n",
       "1444275  [I, can, 't, wait, to, watch, this, virtually,...   \n",
       "\n",
       "                                             commentLemmas  \\\n",
       "2893132  [@jo, williams, exactly, i, see, a, nightmaris...   \n",
       "2599853  [@bugmon, be, you, really, that, obtuse, ?, i,...   \n",
       "125068   [it, 's, so, sad, to, see, trump, 's, impact, ...   \n",
       "2425848  [the, children, ’s, hospital, in, my, city, ha...   \n",
       "1444275  [i, can, 't, wait, to, watch, this, virtually,...   \n",
       "\n",
       "                                        commentNoStopWords  \n",
       "2893132  [@jo, williams, exactly, see, nightmarish, got...  \n",
       "2599853  [@bugmon, really, obtuse, ?, wear, mask, go, ....  \n",
       "125068   ['s, sad, see, trump, 's, impact, nation, ., w...  \n",
       "2425848  [children, ’s, hospital, city, enormous, cash,...  \n",
       "1444275  ['t, wait, watch, virtually, folks, ., grandmo...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments[['commentClean','commentTokens','commentLemmas','commentNoStopWords']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893132    [@jo, williams, exactly, see, nightmarish, got...\n",
       "2599853    [@bugmon, really, obtuse, ?, wear, mask, go, ....\n",
       "125068     ['s, sad, see, trump, 's, impact, nation, ., w...\n",
       "2425848    [children, ’s, hospital, city, enormous, cash,...\n",
       "1444275    ['t, wait, watch, virtually, folks, ., grandmo...\n",
       "                                 ...                        \n",
       "3009694           [best, last, sentence, column, ,, ever, .]\n",
       "592853     [roger, stone, ,, along, paul, manafort, lee, ...\n",
       "1359418    [one, individual, tweet, 798, time, day, ., ’s...\n",
       "4886496          [@anthony, live, journalists, civilians, ?]\n",
       "463789     [believe, senator, klobuchar, ’s, act, share, ...\n",
       "Name: commentNoStopWords, Length: 49865, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments['commentNoStopWords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2179866"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nlp_tools.createCorpus(df_comments['commentNoStopWords'])\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76615"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4   5   6   7   8   9   10  11  12\n",
      "0    0   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "2    0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "3    0   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "4    0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "5    0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "6    0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "7    0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "8    1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "9    0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "10   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
      "11   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "12   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "df = nlp_tools.countVectorizer(df_comments['commentNoStopWords'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    batman  city  exactly  gotham  help  jo  nightmarish  save  see  \\\n",
      "0        0     0        0       0     0   1            0     0    0   \n",
      "1        0     0        0       0     0   0            0     0    0   \n",
      "2        0     0        1       0     0   0            0     0    0   \n",
      "3        0     0        0       0     0   0            0     0    1   \n",
      "4        0     0        0       0     0   0            1     0    0   \n",
      "5        0     0        0       1     0   0            0     0    0   \n",
      "6        0     1        0       0     0   0            0     0    0   \n",
      "7        0     0        0       0     0   0            0     0    0   \n",
      "8        1     0        0       0     0   0            0     0    0   \n",
      "9        0     0        0       0     0   0            0     0    0   \n",
      "10       0     0        0       0     1   0            0     0    0   \n",
      "11       0     0        0       0     0   0            0     1    0   \n",
      "12       0     0        0       0     0   0            0     0    0   \n",
      "13       0     0        0       0     0   0            0     0    0   \n",
      "\n",
      "    superhero  us  usa  williams  \n",
      "0           0   0    0         0  \n",
      "1           0   0    0         1  \n",
      "2           0   0    0         0  \n",
      "3           0   0    0         0  \n",
      "4           0   0    0         0  \n",
      "5           0   0    0         0  \n",
      "6           0   0    0         0  \n",
      "7           0   0    1         0  \n",
      "8           0   0    0         0  \n",
      "9           1   0    0         0  \n",
      "10          0   0    0         0  \n",
      "11          0   0    0         0  \n",
      "12          0   1    0         0  \n",
      "13          0   0    0         0  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "Process text functions we created with wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline here, call functions in package\n",
    "pipeline = [str.lower, remove_stopwords]\n",
    "\n",
    "def prepare(text, pipeline):\n",
    "    tokens = text\n",
    "    for transform in pipeline:\n",
    "        tokens = transform(tokens)\n",
    "    return tokens\n",
    "\n",
    "# Use something like this to process\n",
    "# df['tokens'] = df['text'].apply(prepare, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20324/1890634636.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This is a test of what is possible with ths tester'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tokens = tokenize('This is a test of what is possible with ths tester')\n",
    "counter = Counter(tokens)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean non-essential characters (Remove Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://learning.oreilly.com/library/view/blueprints-for-text/9781492074076/ch04.html#ch04removenoiseregex\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=15):\n",
    "    \"\"\"returns the ratio of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out clean and impurity\n",
    "Want to find noise and the percentage of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Impurity Val: 0.10526315789473684\n",
      "Index Max:2673\n",
      "#IBelieveHer #MeToo\n",
      "\n",
      "\n",
      "\n",
      "#IBelieveHer #MeToo\n"
     ]
    }
   ],
   "source": [
    "pre_impurity = []\n",
    "post_impurity = []\n",
    "clean_comments = []\n",
    "\n",
    "# Step through each of text comments\n",
    "for _ in range(0,len(df_comments_sample)):\n",
    "    # Step through the text comment body row by row\n",
    "    text = df_comments_sample['commentBody'].iloc[_]\n",
    "    pre = nlp_tools.impurity(text) # Call impurity to find non-text characters\n",
    "    clean_text = nlp_tools.clean(text) # Clean up text\n",
    "    post = nlp_tools.impurity(clean_text) # Check Post impurity\n",
    "\n",
    "    pre_impurity.append(pre) # Build list of impurity\n",
    "    post_impurity.append(post) # Build list of post impurity\n",
    "\n",
    "    clean_comments.append(clean_text) # Make list of cleaned text\n",
    "\n",
    "max_val = max(post_impurity) # Find max value\n",
    "ind_max = post_impurity.index(max_val) # find what index max value existed at\n",
    "\n",
    "print('Max Impurity Val: ' + str(max_val)) # Report max impurity\n",
    "print('Index Max:' + str(ind_max))\n",
    "\n",
    "print(df_comments['commentBody'].iloc[ind_max])\n",
    "print('\\n'*2)\n",
    "print(clean_comments[ind_max])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Add new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick normalization of long comment\n",
    "len(df_comments_sample['commentBody'].iloc[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a Foreign Officer I was proud to represent my country overseas. During my 60+ years associated with Eisenhower Fellowships, which has brought over 2,000 outstanding Fellows to the United States, I was proud to see them observe our American democracy. I often would chide my British relatives for not appreciating  the ‘American way.’\\n\\nAfter four years of Trump, I have been handed a sharp comeuppance. We are the laughing stock of the world. Our former traditional allies are dismayed by Trump’s abandonment of core principles of common self interest. Authoritarians such as Putin, Kim, Erdogan, and Mohamed bin Salman are laughing, while they play Trump like an accordion.\\n\\nI recall FDR’s Four Freedoms in 1941. We need Biden reaffirm these freedoms and to stabilize Trump’s shaky ship of state. We must return the United States to the concert of nations that share what had been our decency, our principles, and our alacrity to work with others to protect the oppressed and punish the oppressors.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show that comment\n",
    "df_comments_sample['commentBody'].iloc[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean(df_comments_sample['commentBody'].iloc[800])\n",
    "\n",
    "# text = \"\"\"The café “Saint-Raphaël” is loca-\\nted on Côte dʼAzur.\"\"\"\n",
    "# len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learning.oreilly.com/library/view/blueprints-for-text/9781492074076/ch04.html#idm46749279547768\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a Foreign Officer I was proud to represent my country overseas. During my 60+ years associated with Eisenhower Fellowships, which has brought over 2,000 outstanding Fellows to the United States, I was proud to see them observe our American democracy. I often would chide my British relatives for not appreciating the ‘American way.’ After four years of Trump, I have been handed a sharp comeuppance. We are the laughing stock of the world. Our former traditional allies are dismayed by Trump’s abandonment of core principles of common self interest. Authoritarians such as Putin, Kim, Erdogan, and Mohamed bin Salman are laughing, while they play Trump like an accordion. I recall FDR’s Four Freedoms in 1941. We need Biden reaffirm these freedoms and to stabilize Trump’s shaky ship of state. We must return the United States to the concert of nations that share what had been our decency, our principles, and our alacrity to work with others to protect the oppressed and punish the oppressors.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Foreign Officer I was proud to represent my country overseas. During my 60+ years associated with Eisenhower Fellowships, which has brought over 2,000 outstanding Fellows to the United States, I was proud to see them observe our American democracy. I often would chide my British relatives for not appreciating the 'American way.' After four years of Trump, I have been handed a sharp comeuppance. We are the laughing stock of the world. Our former traditional allies are dismayed by Trump's abandonment of core principles of common self interest. Authoritarians such as Putin, Kim, Erdogan, and Mohamed bin Salman are laughing, while they play Trump like an accordion. I recall FDR's Four Freedoms in 1941. We need Biden reaffirm these freedoms and to stabilize Trump's shaky ship of state. We must return the United States to the concert of nations that share what had been our decency, our principles, and our alacrity to work with others to protect the oppressed and punish the oppressors.\n"
     ]
    }
   ],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalize(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use normalization\n",
    "normalized_col = df_comments_sample['commentBody'].map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Identifiers like URLs, email, or phone numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.preprocessing.resources import RE_URL\n",
    "\n",
    "count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cafe', 'Saint', 'Raphael', 'is', 'located', 'on', 'Cote', 'd', 'Azur']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenization\n",
    "tokens = tokenizer.tokenize(normalize(text))\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "# count_words(df_comments_sample, column='commentBody', preprocess=RE_URL.findall).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecaab7e4f2054dc1b3b08bfc690ea1c3d11e0c83265adf695ac1706ca10c6a57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
